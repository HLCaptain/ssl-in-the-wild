{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uduEflDOeW9k"
   },
   "source": [
    "# **Használat előtt:**\n",
    "\n",
    "A gyökérkönyvtárban található `kaggle.json` fájlt be kell másolni a futattás előtt, kicserélve a megfelelő helyeken a <> közötti részt a felhasználónév és API kulcs párra."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_5vQrPHXqmB2"
   },
   "source": [
    "# Keretrendszerbe beépített kódrészletek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rJoW8MJxeoWg",
    "outputId": "053a7a33-7dfc-4556-ca7b-4ca69617c538"
   },
   "outputs": [],
   "source": [
    "!pip install opendatasets --quiet\n",
    "!pip install pandas --quiet\n",
    "!pip install torch --quiet\n",
    "!pip install lightning --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-a_9mwkWhbYb"
   },
   "source": [
    "A használt keretrendszerbe beépített adatbetöltő és előfeldolgozó fájl `src/data/birds_datamodule.py` az alábbiakat tartalmazza, apróbb változtatásokkal. Az itteni változtatások az osztálynak a belső változóinak külső elérhetővé tételére szolgálnak."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yQQiEOg-edC1"
   },
   "outputs": [],
   "source": [
    "from typing import Any, Dict, Optional, Tuple\n",
    "\n",
    "import torch\n",
    "from lightning import LightningDataModule\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Dataset, random_split\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import opendatasets as od\n",
    "from PIL import Image\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsdujtc_dODF"
   },
   "outputs": [],
   "source": [
    "\"\"\" Created classlist by a csv file.\n",
    ":param class_csv: The class list file path.\n",
    "\"\"\"\n",
    "def find_class_list(class_csv: str):\n",
    "    # read file\n",
    "    birds_100_csv = pd.read_csv(class_csv)\n",
    "    # select unique from labels list\n",
    "    classes = birds_100_csv['labels'].unique()\n",
    "    class_to_idx = {cls_name: i for i, cls_name in enumerate(classes)}\n",
    "    return classes, class_to_idx\n",
    "\n",
    "class ImageFolderCustom(Dataset):\n",
    "    \"\"\" Costum ImageFolder. Original code from https://www.learnpytorch.io/04_pytorch_custom_datasets/.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, class_csv: str, targ_dir: str, transform=None) -> None:\n",
    "        \"\"\" Initialize a ImageFolderCustom with a targ_dir and transform (optional) parameter.\n",
    "\n",
    "        :param class_csv: The class list file path.\n",
    "        :param targ_dir: The images directory.\n",
    "        :param transform: Image transformation steps.\n",
    "        \"\"\"\n",
    "        # Get all image paths\n",
    "        self.paths = list(Path(targ_dir).glob(\"*/*.jpg\"))\n",
    "        # Setup transforms\n",
    "        self.transform = transform\n",
    "        # Create classes and class_to_idx attributes\n",
    "        self.classes, self.class_to_idx = find_class_list(class_csv)\n",
    "\n",
    "    def load_image(self, index: int) -> Image.Image:\n",
    "        \"\"\" Load images by indexes from paths.\n",
    "\n",
    "        :param index: The image path index\n",
    "        \"\"\"\n",
    "        image_path = self.paths[index]\n",
    "        return Image.open(image_path)\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\" Returns the total number of samples.\n",
    "        \"\"\"\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Tuple[torch.Tensor, int]:\n",
    "        \"\"\" Returns one sample of data, data and label (X, y).\n",
    "        :param index: The image path index\n",
    "        \"\"\"\n",
    "        img = self.load_image(index)\n",
    "        class_name  = self.paths[index].parent.name # expects path in data_folder/class_name/image.jpeg\n",
    "        class_idx = self.class_to_idx[class_name]\n",
    "\n",
    "        # Transform if necessary\n",
    "        if self.transform:\n",
    "          return self.transform(img), class_idx # return data, label (X, y)\n",
    "        else:\n",
    "          return img, class_idx # return data, label (X, y)\n",
    "\n",
    "class BirdsDataModule(LightningDataModule):\n",
    "    \"\"\"`LightningDataModule` for the BIRDS 525 SPECIES- IMAGE CLASSIFICATION dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir: str = data_dir,\n",
    "        train_val_test_split: Tuple[float, float, float] = (0.8, 0.1, 0.1),\n",
    "        batch_size: int = 64,\n",
    "        num_workers: int = 0,\n",
    "        pin_memory: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"Initialize a `BirdsDataModule`.\n",
    "\n",
    "        :param data_dir: The data directory. Defaults to `\"data/\"`.\n",
    "        :param batch_size: The batch size. Defaults to `64`.\n",
    "        :param num_workers: The number of workers. Defaults to `0`.\n",
    "        :param pin_memory: Whether to pin memory. Defaults to `False`.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        # this line allows to access init params with 'self.hparams' attribute\n",
    "        # also ensures init params will be stored in ckpt\n",
    "        self.save_hyperparameters(logger=False)\n",
    "\n",
    "        # data transformations\n",
    "        self.transforms = transforms.Compose(\n",
    "            [transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))]\n",
    "        )\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.data_train: Optional[Dataset] = None\n",
    "        self.data_val: Optional[Dataset] = None\n",
    "        self.data_test: Optional[Dataset] = None\n",
    "\n",
    "        self.batch_size_per_device = batch_size\n",
    "\n",
    "    @property\n",
    "    def num_classes(self) -> int:\n",
    "        \"\"\"Get the number of classes.\n",
    "\n",
    "        :return: The number of Birds dataset classes.\n",
    "        \"\"\"\n",
    "        classes, _ = find_class_list(self.data_dir+'100-bird-species/birds.csv')\n",
    "        return len(classes)\n",
    "\n",
    "    def prepare_data(self) -> None:\n",
    "        \"\"\"Download data if needed. Lightning ensures that `self.prepare_data()` is called only\n",
    "        within a single process on CPU, so you can safely add your downloading logic within. In\n",
    "        case of multi-node training, the execution of this hook depends upon\n",
    "        `self.prepare_data_per_node()`.\n",
    "\n",
    "        Do not use it to assign state (self.x = y).\n",
    "        \"\"\"\n",
    "        if not os.path.isdir(self.data_dir+'100-bird-species/'):\n",
    "            od.download(\n",
    "                \"https://www.kaggle.com/datasets/gpiosenka/100-bird-species\", data_dir=self.data_dir)\n",
    "\n",
    "    def setup(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Load data. Set variables: `self.data_train`, `self.data_val`, `self.data_test`.\n",
    "\n",
    "        This method is called by Lightning before `trainer.fit()`, `trainer.validate()`, `trainer.test()`, and\n",
    "        `trainer.predict()`, so be careful not to execute things like random split twice! Also, it is called after\n",
    "        `self.prepare_data()` and there is a barrier in between which ensures that all the processes proceed to\n",
    "        `self.setup()` once the data is prepared and available for use.\n",
    "\n",
    "        :param stage: The stage to setup. Either `\"fit\"`, `\"validate\"`, `\"test\"`, or `\"predict\"`. Defaults to ``None``.\n",
    "        \"\"\"\n",
    "        # Divide batch size by the number of devices.\n",
    "        if self.trainer is not None:\n",
    "            if self.hparams.batch_size % self.trainer.world_size != 0:\n",
    "                raise RuntimeError(\n",
    "                    f\"Batch size ({self.hparams.batch_size}) is not divisible by the number of devices ({self.trainer.world_size}).\"\n",
    "                )\n",
    "            self.batch_size_per_device = self.hparams.batch_size // self.trainer.world_size\n",
    "\n",
    "        # load and split datasets only if not loaded already\n",
    "        if not self.data_train and not self.data_val and not self.data_test:\n",
    "            image_dir_path = Path(self.data_dir + \"100-bird-species/\")\n",
    "            birds_100_csv_path = image_dir_path / \"birds.csv\"\n",
    "            train_dir = image_dir_path / \"train\"\n",
    "            test_dir = image_dir_path / \"test\"\n",
    "            val_dir = image_dir_path / \"valid\"\n",
    "            self.trainset = ImageFolderCustom(class_csv=birds_100_csv_path, targ_dir=train_dir,transform=self.transforms)\n",
    "            self.testset = ImageFolderCustom(class_csv=birds_100_csv_path, targ_dir=test_dir, transform=self.transforms)\n",
    "            self.valset = ImageFolderCustom(class_csv=birds_100_csv_path, targ_dir=val_dir, transform=self.transforms)\n",
    "            dataset = ConcatDataset(datasets=[self.trainset, self.testset, self.valset])\n",
    "            self.data_train, self.data_val, self.data_test = random_split(\n",
    "                dataset=dataset,\n",
    "                lengths=self.hparams.train_val_test_split,\n",
    "                generator=torch.Generator().manual_seed(42),\n",
    "            )\n",
    "\n",
    "    def train_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the train dataloader.\n",
    "\n",
    "        :return: The train dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_train,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the validation dataloader.\n",
    "\n",
    "        :return: The validation dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_val,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def test_dataloader(self) -> DataLoader[Any]:\n",
    "        \"\"\"Create and return the test dataloader.\n",
    "\n",
    "        :return: The test dataloader.\n",
    "        \"\"\"\n",
    "        return DataLoader(\n",
    "            dataset=self.data_test,\n",
    "            batch_size=self.batch_size_per_device,\n",
    "            num_workers=self.hparams.num_workers,\n",
    "            pin_memory=self.hparams.pin_memory,\n",
    "            shuffle=False,\n",
    "        )\n",
    "\n",
    "    def teardown(self, stage: Optional[str] = None) -> None:\n",
    "        \"\"\"Lightning hook for cleaning up after `trainer.fit()`, `trainer.validate()`,\n",
    "        `trainer.test()`, and `trainer.predict()`.\n",
    "\n",
    "        :param stage: The stage being torn down. Either `\"fit\"`, `\"validate\"`, `\"test\"`, or `\"predict\"`.\n",
    "            Defaults to ``None``.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    def state_dict(self) -> Dict[Any, Any]:\n",
    "        \"\"\"Called when saving a checkpoint. Implement to generate and save the datamodule state.\n",
    "\n",
    "        :return: A dictionary containing the datamodule state that you want to save.\n",
    "        \"\"\"\n",
    "        return {}\n",
    "\n",
    "    def load_state_dict(self, state_dict: Dict[str, Any]) -> None:\n",
    "        \"\"\"Called when loading a checkpoint. Implement to reload datamodule state given datamodule\n",
    "        `state_dict()`.\n",
    "\n",
    "        :param state_dict: The datamodule state returned by `self.state_dict()`.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hjm3w4R_iePr"
   },
   "source": [
    "# Teszt futattás\n",
    "Az osztály meghívása, és a megfelelő metódusok futattása. A keretrendszer szintén ebben a sorrenben végzi el a futtatásokat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1jdClHlCgrxO"
   },
   "outputs": [],
   "source": [
    "module = BirdsDataModule(data_dir = data_dir,\n",
    "                        batch_size= 64,\n",
    "                        train_val_test_split= [0.8, 0.1, 0.1],\n",
    "                        num_workers= 0,\n",
    "                        pin_memory= False)\n",
    "module.prepare_data() #szükséges a .json fájl tartalma, vagy a futatás során bekéri a felhasználónév + API kulcs párt\n",
    "module.setup()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xmc-BFaQi8GQ"
   },
   "source": [
    "# Adatkészlet bemutatása"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gYkH3Bv_isTy"
   },
   "source": [
    "A train, test és validációs adatkészletek létrehozása példaként."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tbajmsy9eSZk",
    "outputId": "332c38f5-042b-4aaa-8a12-b2214f16a72d"
   },
   "outputs": [],
   "source": [
    "trainDL = module.train_dataloader()\n",
    "valDL = module.val_dataloader()\n",
    "testDL = module.test_dataloader()\n",
    "trainDL, valDL, testDL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CILveagqi0ti",
    "outputId": "7fce9801-3b6a-48a6-d966-9095dfda145b"
   },
   "outputs": [],
   "source": [
    "len(trainDL), len(valDL), len(testDL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3WCVoO_df20t"
   },
   "outputs": [],
   "source": [
    "class_dict = module.trainset.class_to_idx # cimke név - azonosító pár\n",
    "class_names = module.trainset.classes     # cimke név lista"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFDxdBLGqdLz"
   },
   "source": [
    "**Különböző képek teszt adatai**\n",
    "\n",
    "Az adatkészletben 80000-nél több kép van, ebből párat megadtunk előre, de másikakat is lehetséges megtekinteni, létrehozva a változót az alábbi minták szerint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyedEhEsqcnT"
   },
   "outputs": [],
   "source": [
    "data0 = [module.trainset[729][0], module.trainset[729][1]]\n",
    "data1 = [module.trainset[3000][0], module.trainset[3000][1]]\n",
    "data2 = [module.trainset[3700][0], module.trainset[3700][1]]\n",
    "data3 = [module.trainset[15967][0], module.trainset[15967][1]]\n",
    "data4 = [module.trainset[40729][0], module.trainset[40729][1]]\n",
    "data5 = [module.trainset[1567][0], module.trainset[1567][1]]\n",
    "data6 = [module.trainset[0][0], module.trainset[0][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mIMTNFl4r9YR"
   },
   "outputs": [],
   "source": [
    "# A general_image érétkét kell a kívántaknak megfelleően megadni. Kiválaszthatók a fentebbi adatok vagy [Kép Tensor, felirat szám] formátumban is megadható.\n",
    "general_image = data0\n",
    "img, label = general_image[0], general_image[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RKBCgQWVfgLq",
    "outputId": "d0e766a9-34a3-4168-e08e-4740931b954f"
   },
   "outputs": [],
   "source": [
    "print(f\"Image shape: {img.shape}\")\n",
    "print(f\"Image datatype: {img.dtype}\")\n",
    "print(f\"Image label id: {label}\")\n",
    "print(f\"Image label: {class_names[label]}\")\n",
    "print(f\"Label datatype: {type(label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y2jzHxsJhXpA"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def display_images(image_data: list):\n",
    "\n",
    "    plt.figure(figsize=(5, 3))\n",
    "    targ_image, targ_label = image_data[0], image_data[1]\n",
    "    targ_image_adjust = targ_image.permute(1, 2, 0) # [color_channels, height, width] -> [height, width, color_channels]\n",
    "    plt.imshow(targ_image_adjust)\n",
    "    plt.axis(\"off\")\n",
    "    title = f\"class: {class_names[targ_label]}\"\n",
    "    plt.title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 327
    },
    "id": "DOEQHgEZ65TZ",
    "outputId": "c9c3bdeb-9f7c-4652-91c9-433bf4c40975"
   },
   "outputs": [],
   "source": [
    "display_images(general_image)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_5vQrPHXqmB2",
    "hjm3w4R_iePr"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
